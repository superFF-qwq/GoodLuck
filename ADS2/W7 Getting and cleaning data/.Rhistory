#g1=g+sm
#g1
g=g+geom_smooth(method="lm",mapping=aes(x=as.numeric(clarity)))
g=g+scale_y_continuous(limit=c(0.3,3.0),minor_breaks=0.5)
#ggarrange(g1,g2)
g
#dev.off()
library(tidyverse)
#library("gridExtra")
library(ggplot2)
#install.packages('ppubr')
#library(ppubr)
dev.off()
dev.new()
data(diamonds)
#data=diamonds
#head(diamonds)
#g=ggplot(data=diamonds,mapping=
#         aes(x=carat,y=price,group=cut))
#g
#g1=g+geom_point(stat="identity",
#                aes(colour=cut))
#g1=g+geom_point(stat="identity",
#                aes(colour=cut),size=0.1,alpha=0.2)
#g1=g+geom_point(stat="identity",
#                aes(colour=cut),size=0.1,alpha=0.2,shape=18)
#alpha:transparency
#d=ggplot(diamonds,aes(carat))+xlim(0,3)
#d+stat_bin(aes(y=after_stat(density)),
#           binwidth=0.1,geom="area")
#png(file="fig2.png")
#d+stat_bin(aes(y=..count..),
#           binwidth=0.01,geom="area")
#d+stat_bin(aes(size=..density..),
#           binwidth=0.1,geom="point",
#           )
#png(file="purple.png")
#d+geom_area(stat="count")
g=ggplot(data=diamonds,mapping=
aes(x=clarity,y=carat,color=cut))
g=g+geom_boxplot()
g=g+labs(title="Carat vs clarity")
#g=g+ggtitle("Carat vs clarity")
#g=g+
sm=geom_smooth(method="lm",mapping=aes(x=as.numeric(clarity),
y=clarity))
#g=g+facet_wrap(cut~color)
#g=g+facet_grid(cut~color)
#g=g+scale_color_brewer(rgb(red=1,green=0.1,blue=1))
g=g+scale_color_brewer(palette="Purples")
g1=g+theme(plot.title=element_text(hjust=0.5)) #title in the middle
#task4
#g=g+scale_y_log10()
g=g+scale_y_continuous(trans="log10")
#g=g+scale_y_continuous(trans="log10",name="log10 carot")
g=g+labs(y="carot/log10(carat)")
#g=g+theme()
#g=g+theme(ylab="log10 carat")
#g
#g1=g+sm
#g1
g=g+geom_smooth(method="lm",mapping=aes(x=as.numeric(clarity)))
g=g+scale_y_continuous(limit=c(0.3,3.0),width=0.5)
library(tidyverse)
library(tidyverse)
#library("gridExtra")
#library(ggplot2)
library(cowplot)
install.packages('rmarkdown')
install.packages('rmarkdown')
library(rmarkdown)
install.packages("rmarkdown")
install.packages('tinytex')
install.packages('tinytex')
version
tinytex::install_tinytex()
var1 = c(2,3,5,6)
class(var1)
var2 = c(2.0,3.9,5.1,6.9)
class(var2)
var3 = c(2L,3L,5L,6L)
class(var3)
setwd("E:/A大学/大二上/ADS2/week7")
rm(list = ls())
dev.off()
rm(list = ls())
setwd("E:/A大学/大二上/ADS2/week7")
rm(list = ls())
setwd("E:\\A大学\\大二上\\ADS2\\week7")
rm(list = ls())
setwd("E:\\A大学\\大二上\\ADS2\\Week 7 Getting and cleaning data")
wnv = read.csv("Week_7_WNV_mosquito_test_results.csv")
library(tidyverse)
#rename(file,YEARS = SEASON.YEAR)
#1.1. Explore our data
#data structure
class(wnv)
head(wnv)
class(wnv$TEST.DATE)
# Let's try to make it a real date/time type
wnv$TEST.DATE = as.POSIXct(wnv$TEST.DATE, format = "%m/%d/%Y %H:%M:%S",
tz = "America/Chicago")
#class(wmv$TEST.DATE)
#head(wmv$TEST.DATE)
#Let's try to convert timezone of daytime type data
dat1 = wnv$TEST.DATE[1]
dat1
attributes(dat1)$tzone
attributes(dat1)$tzone = "America/Los_Angeles"
dat1
#Let's try to separate LOCATION column into LATITUDE and LONGITUDE
head(wnv$LOCATION, 1)
#gsub(pattern, replacement, x)
head(wnv, 1)
wnv$LOCATION = gsub("[()]", "", wnv$LOCATION, perl = T)
# gsub()函数用于替换字符串中的指定模式（正则表达式）为新的字符串。
# gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE,
# useBytes = FALSE)
# pattern: 要匹配和替换的正则表达式模式。
# replacement: 替换模式。可以是一个字符串，也可以是一个函数。
# x: 要在其中进行替换操作的输入字符串或字符向量。
# ignore.case: 一个逻辑值，指定是否忽略模式匹配时的大小写，默认为FALSE。
# perl: 一个逻辑值，指定是否使用Perl风格的正则表达式，默认为FALSE。
# fixed: 一个逻辑值，指定是否将pattern视为普通字符串而不是正则表达式，默认为FALSE。
# useBytes: 一个逻辑值，指定是否按字节而不是字符处理输入字符串，默认为FALSE。
wnv = separate(
wnv,
LOCATION,
into = c("LATITUDE", "LONGITUDE"),
sep = ",",
remove = F,
fill = "left",
convert = T
)
## separate() convert a long format into a wide format
## separate(data, col, into, sep = "[^[:alnum:]]+", remove = TRUE, convert = FALSE)
## data: 需要拆分的数据框或数据集
## col: 需要进行拆分操作的列名或列索引
## into: 指定新的列名，可以是字符向量或一个整数，表示需要创建的新列数量
## sep: 用于指定拆分每个元素的分隔符。默认为非字母数字字符
## remove: 一个逻辑值，用于指定是否删除原始列。默认为TRUE，表示删除
## convert: 一个逻辑值，用于指定是否将拆分出的新列转换为数值类型，默认为FALSE
head(wnv, 1)
summary(wnv)
#### There other useful ways to do string manipulations ######
library (stringr) # for example.
#### explore other functions
# str_sub(), str_replace() , str_split()
#1.2 Screen and diagnosis.
# Just for fun / practice purpose, let's try the functions gather and spread.
# In order not to mess up the original dataset, make a new transformed dataset
# first, let's load the library for this.
#library(tidy)
wnv2 = gather(wnv)
# gather函数的功能是将宽数据准换为长数据
# 函数形式：gather(data,key,value,...,na.rm=F)
# key：将原数据框中的所有列赋给一个新变量key
# value：将原数据框中的所有值赋给一个新变量value
# …：指定变量聚到同一列中，可通过变量在数据集中的列数，也可以使用"-"排除特定的变量
# na.rm：是否删除缺失值
wnv2
str(wnv2) # This data as it is too messy. We will remove it.
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
print(paste("为了有90%的机会选择每个基因至少一次，我们需要处理大约", round(n_needed), "只果蝇。"))
n = 14000
p = 1/14000
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
print(paste("为了有90%的机会选择每个基因至少一次，我们需要处理大约", round(n_needed), "只果蝇。"))
n = 14000
p = 1/14000
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
print(paste("为了有90%的机会选择每个基因至少一次，我们需要处理大约", round(n_needed), "只果蝇。"))
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
print(paste("为了有90%的机会选择每个基因至少一次，我们需要处理大约", round(n_needed), "只果蝇。"))
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x), probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
probability <- function(n, p) {
dbinom(k = 1, size = n, prob = p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x), probability(x, p = 1/14e3))
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
probability <- function(n, p) {
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
print(paste("为了有90%的机会选择每个基因至少一次，我们需要处理大约", round(n_needed), "只果蝇。"))
probability <- function(n, p) {
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 5e4, by = 5)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
n_needed <- n[prob > 0.9][1]
round(n_needed)
prob
tail(prob)
max(prob)
probability <- function(n, p) {
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 1e6, by = 100)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
probability <- function(n, p) {
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
n <- seq(from = 5, to = 1e7, by = 100)
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
1-(13999/14000)^14000
n = 1e9
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
n = 1e9
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
n = 1e8
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
n = 1e7
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
probability <- function(n, p) {
n
p
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
#n <- seq(from = 5, to = 1e7, by = 100)
n = 1e7
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
probability <- function(n, p) {
writelines(n)
writelines(p)
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
#n <- seq(from = 5, to = 1e7, by = 100)
n = 1e7
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
probability <- function(n, p) {
print(n)
print(p)
dbinom(1, n, p)
}
probability <- function(n, p) {
print(n)
print(p)
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
#n <- seq(from = 5, to = 1e7, by = 100)
n = 1e7
prob <- sapply(n, function(x) probability(x, p = 1/14e3))
max(prob)
n = 1e5
prob <- sapply(n, function(x) probability(x, p = 0.2))
max(prob)
n_needed <- n[prob > 0.9][1]
round(n_needed)
probability <- function(n, p) {
print(n)
print(p)
dbinom(1, n, p)
}
# 调整n的值使得概率为0.9
#n <- seq(from = 5, to = 1e7, by = 100)
n = 1e5
prob <- sapply(n, function(x) probability(x, p = 0.2))
max(prob)
setwd("E:/A大学/大二上/ADS2/week7")
load("E:/A大学/大二上/ADS2/Week 7 Getting and cleaning data/pgp3_2023_modified.RData")
View(pgp3)
View(pgp3)
setwd("E:\\A大学\\大二上\\ADS2\\Week 7 Getting and cleaning data")
#####
####### Practice 2
########
# HINT: ELISA kit.
# Op stands for "optical density" 光密度，吸光度，反应目的蛋白含量
# First, let's clean up the environment.
rm(list = ls())
dev.off()
setwd("E:\\A大学\\大二上\\ADS2\\Week 7 Getting and cleaning data")
#####
####### Practice 2
########
# HINT: ELISA kit.
# Op stands for "optical density" 光密度，吸光度，反应目的蛋白含量
# First, let's clean up the environment.
#rm(list = ls())
#dev.off()
# Load the datatable. This time, it is a tab separate txt file.
# Therefore we used read.table rather than read.csv.
pgp3_og = read.table("Week_7_Tests_PGP3.txt", sep = '\t', header = T)
# Have a look at the data and get a feeling.
head(pgp3_og)
summary(pgp3_og)
str(pgp3_og)
table(pgp3_og$measured)
table(pgp3_og$SampleID)
which(duplicated(pgp3_og)) # ok there are duplicated rows.
idx1 = which(duplicated(pgp3_og))
idx2 = which(duplicated(pgp3_og, fromLast = T))
# fromLast = T look for duplicated from right to left,
# the first one will be thought as FALSE
# duplicated() returns a list of TRUE/FALSE
# which() returns the index
idx1
idx2
# Since it only gives one set of index duplicated rather than both.
# Let's check whether duplication is true.
pgp3_og[c(idx1, idx2),]
# it is safe to keep only one copy,so then we can
# remove the duplicated in the following
### duplicated() will only give you the duplicated rows,
### BUT not the original rows, so we need fromlast = T
### to the next line to get the originals
# We already saw that some data seems to be missing.
anyNA(pgp3_og)
# There are missing values.
pgp3_og[which(!complete.cases(pgp3_og)),]
pgp3_og[!complete.cases(pgp3_og),]
length(rownames(pgp3_og[complete.cases(pgp3_og),]))
# 2219, less than number of rows that is 2324.
####
#### Treatment.
#1. it is safe to keep only one set of duplicated rows.
pgp3 = pgp3_og[-idx1,]# remove one set of the duplicated rows.
# Let's check if all the duplicated rows are removed.
which(duplicated(pgp3)) # ok.
#2. transform the long format to a wide format.
pgp3 = spread (pgp3, measured, value)
# spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)
# key：指定转换的某列，其观测值作为转换后的列名
# value：其他列的观测值分散到相对应的各个单元
# fill：对于缺失值，可将fill的值赋值给被转型后的缺失值
head(pgp3)
load("E:/A大学/大二上/ADS2/Week 7 Getting and cleaning data/pgp3_2023_modified.RData")
View(pgp3)
View(pgp3)
setwd("E:\\A大学\\大二上\\ADS2\\Week 7 Getting and cleaning data")
#####
####### Practice 2
########
# HINT: ELISA kit.
# Op stands for "optical density" 光密度，吸光度，反应目的蛋白含量
# First, let's clean up the environment.
#rm(list = ls())
#dev.off()
# Load the datatable. This time, it is a tab separate txt file.
# Therefore we used read.table rather than read.csv.
pgp3_og = read.table("Week_7_Tests_PGP3.txt", sep = '\t', header = T)
# Have a look at the data and get a feeling.
head(pgp3_og)
summary(pgp3_og)
str(pgp3_og)
table(pgp3_og$measured)
table(pgp3_og$SampleID)
which(duplicated(pgp3_og)) # ok there are duplicated rows.
idx1 = which(duplicated(pgp3_og))
idx2 = which(duplicated(pgp3_og, fromLast = T))
# fromLast = T look for duplicated from right to left,
# the first one will be thought as FALSE
# duplicated() returns a list of TRUE/FALSE
# which() returns the index
idx1
idx2
# Since it only gives one set of index duplicated rather than both.
# Let's check whether duplication is true.
pgp3_og[c(idx1, idx2),]
# it is safe to keep only one copy,so then we can
# remove the duplicated in the following
### duplicated() will only give you the duplicated rows,
### BUT not the original rows, so we need fromlast = T
### to the next line to get the originals
# We already saw that some data seems to be missing.
anyNA(pgp3_og)
# There are missing values.
pgp3_og[which(!complete.cases(pgp3_og)),]
pgp3_og[!complete.cases(pgp3_og),]
length(rownames(pgp3_og[complete.cases(pgp3_og),]))
# 2219, less than number of rows that is 2324.
####
#### Treatment.
#1. it is safe to keep only one set of duplicated rows.
pgp3 = pgp3_og[-idx1,]# remove one set of the duplicated rows.
# Let's check if all the duplicated rows are removed.
which(duplicated(pgp3)) # ok.
#2. transform the long format to a wide format.
pgp3 = spread (pgp3, measured, value)
# spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)
# key：指定转换的某列，其观测值作为转换后的列名
# value：其他列的观测值分散到相对应的各个单元
# fill：对于缺失值，可将fill的值赋值给被转型后的缺失值
head(pgp3)
summary(pgp3)
str(pgp3)
table(pgp3$sex)  # hard to understand
table(as.character(pgp3$sex))
# 1 and 2. From the source of data we know 1 is male and 2 is female.
#3. Data types
#3.1 The SampleID should be factors, NOT integers.
#3.2 The elisa od values shouldn't be factors. Change them to numeric values.
#3.3 The sex variable is not easily interpretable. change them to M and F.
#covert SampleID, age.F, sex to readable factor
pgp3$SampleID = as.character(pgp3$SampleID)
pgp3$elisa.od = as.numeric(as.character(pgp3$elisa.od))
# Here, we change the factor to character first.
# What would happen if you don't convert to character first?
# 一个常见问题是把factor转换成numeric。
# 比如表示年份的变量，被编码为factor类型的年份变量，需要转换成numeric用于计算。
# 要点是，直接对factor变量使用as.numeric没有意义和不可控，会导致系统采用隐式转换。
pgp3$elisa.pre.od = as.numeric(as.character(pgp3$elisa.pre.od))
pgp3$sex = gsub("1", "M", as.character(pgp3$sex))
pgp3$sex = gsub("2", "F", as.character(pgp3$sex))
# revision: similar to w7-p1, gsub(pattern, replacement, string)
pgp3$sex = as.factor(pgp3$sex)
summary(pgp3)
#4. Missing values.
# Shall we remove them? decided to drop them out.
length(rownames(pgp3[!complete.cases(pgp3),]))
# 103, as in the summary for sex values missing.
pgp3 = pgp3[complete.cases(pgp3),]
anyNA(pgp3)
#5. Any strange patterns?
ggplot(data = pgp3) + geom_boxplot(aes(x = sex, y = elisa.od))
ggplot(data = pgp3) + geom_boxplot(aes(x = age.f, y = elisa.pre.od))
# What is going on? The x-axis label of the first group is missing.
# These are not dropped since it is not recorded as NA values.
# These should be removed,
# considering we are interested in the relationship between age and elisa od.
head(pgp3[pgp3$age.f == '',])
pgp3 = pgp3[pgp3$age.f != '', ]
ggplot(data = pgp3) + geom_boxplot(aes(x = age.f, y = elisa.pre.od))
ggplot(data = pgp3) + geom_boxplot(aes(x = age.f, y = elisa.od))
#6.  We want to reshape the dataframe so that we could compare ELISA od
# at two time points at each age group.
# For that purpose, we should reshape the dataframe again.
head(pgp3)
head(pgp3)
pgp3 = gather(
pgp3,
key = "time.point",
value = "ELISA.od",
elisa.od:elisa.pre.od,
factor_key = T
)
# gather(data=,key=,value=,...,na.rm=,convert=,factor_key=)
# key：创建一个新的列名，原数据的旧列名成为新列名的观测值
# value：再创建一个新的列名，原数据的所有旧列名的观测值成为新列名的观测值
# ...：按照实际需要自行指定需要转换的列
# na.rm：逻辑值，是否删除缺失值
# convert：逻辑值，在key列是否进行数据类型转换
# factor_key:逻辑值，若是F，则key自动转换为字符串，反之则是因子，level不变
head(pgp3)
load("E:/A大学/大二上/ADS2/Week 7 Getting and cleaning data/pgp3_2023_modified.RData")
a = c(1,2,3)
b = c(4,5,6)
wilcox.test(a,b, alternative = 'greater',paired = F )
Field_A<- c(10.2,  10.7,  15.5,  10.4 , 9.9,  10.0,  16.6,  15.1,  15.2,  13.8,  14.1 , 11.4 , 11.5 , 11.0)
Field_B<-  c( 8.1,  8.5,  8.4 , 7.3,  8.0,  7.1,  13.9 , 12.2,  13.4,  11.3,  12.6 , 12.6,  12.7 , 12.4 , 11.3,  12.5)
wilcox.test(Field_A, Field_B, alternative = 'greater',paired = F )
